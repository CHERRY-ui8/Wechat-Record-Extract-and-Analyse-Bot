import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import os

class KeyMomentsAnalyzer:
    def __init__(self, chat_file: str, user_name: str, partner_name: str):
        self.chat_file = chat_file
        self.user_name = user_name
        self.partner_name = partner_name
        self.messages = []  # æ·»åŠ  messages å±æ€§
        self.key_dates = {
            'relationship_start': None,  # æ­£å¼ç¡®å®šå…³ç³»çš„æ—¶é—´
            'conflicts': [],             # åµæ¶çš„æ—¶é—´ç‚¹
            'special_days': {            # ç‰¹æ®Šæ—¥å­
                'anniversary': None,     # çºªå¿µæ—¥
                'valentine': [],         # æƒ…äººèŠ‚
                'qixi': [],             # ä¸ƒå¤•èŠ‚
            }
        }
        self.landmark_topics = {
            'terms_of_endearment': [],   # äº²æ˜µç§°å‘¼
            'literature': [],            # æ–‡å­¦ä½œå“è®¨è®º
            'movies': [],               # ç”µå½±è®¨è®º
            'tv_shows': [],             # ç”µè§†å‰§è®¨è®º
            'social_topics': [],        # ç¤¾ä¼šè¯é¢˜
            'intimate_topics': []       # äº²å¯†è¯é¢˜
        }
        
    def set_key_date(self, date_type: str, date: str, description: str = ""):
        """è®¾ç½®å…³é”®æ—¶é—´èŠ‚ç‚¹"""
        if date_type == 'relationship_start':
            self.key_dates['relationship_start'] = {
                'date': datetime.strptime(date, '%Y-%m-%d'),
                'description': description
            }
        elif date_type == 'conflict':
            self.key_dates['conflicts'].append({
                'date': datetime.strptime(date, '%Y-%m-%d'),
                'description': description
            })
        elif date_type in ['anniversary', 'valentine', 'qixi']:
            if date_type == 'anniversary':
                self.key_dates['special_days']['anniversary'] = {
                    'date': datetime.strptime(date, '%Y-%m-%d'),
                    'description': description
                }
            else:
                self.key_dates['special_days'][date_type].append({
                    'date': datetime.strptime(date, '%Y-%m-%d'),
                    'description': description
                })

    def analyze_attitude_changes(self, messages: List[Dict[str, Any]], 
                               key_date: datetime, 
                               days_before: int = 7, 
                               days_after: int = 7) -> Dict[str, Any]:
        """åˆ†æå…³é”®æ—¶é—´ç‚¹å‰åçš„æ€åº¦å˜åŒ–"""
        start_date = key_date - timedelta(days=days_before)
        end_date = key_date + timedelta(days=days_after)
        
        before_messages = []
        after_messages = []
        
        for msg in messages:
            msg_date = msg['timestamp']  # ç›´æ¥ä½¿ç”¨æ—¶é—´æˆ³ï¼Œä¸éœ€è¦è½¬æ¢
            if start_date <= msg_date < key_date:
                before_messages.append(msg)
            elif key_date <= msg_date <= end_date:
                after_messages.append(msg)
        
        return {
            'before': self._analyze_attitude(before_messages),
            'after': self._analyze_attitude(after_messages),
            'change': self._compare_attitudes(
                self._analyze_attitude(before_messages),
                self._analyze_attitude(after_messages)
            )
        }

    def _analyze_attitude(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ¶ˆæ¯ä¸­çš„æ€åº¦"""
        user_messages = [msg for msg in messages if msg['is_user']]
        partner_messages = [msg for msg in messages if not msg['is_user']]
        
        # åˆ†ææƒ…æ„Ÿè¯æ±‡ä½¿ç”¨
        positive_words = ['å–œæ¬¢', 'çˆ±', 'å¼€å¿ƒ', 'é«˜å…´', 'å¹¸ç¦', 'å¥½', 'æ£’', 'ç¾', 'èµ', 'å¯çˆ±']
        negative_words = ['è®¨åŒ', 'ç”Ÿæ°”', 'éš¾è¿‡', 'ä¼¤å¿ƒ', 'ä¸å¥½', 'çƒ¦', 'ç´¯', 'å›°', 'å¿™', 'çƒ¦']
        neutral_words = ['å—¯', 'å“¦', 'å¥½', 'è¡Œ', 'å¯ä»¥', 'çŸ¥é“', 'æ˜ç™½', 'äº†è§£']
        
        def analyze_word_usage(messages):
            content = ' '.join(msg['content'] for msg in messages)
            return {
                'positive_count': sum(1 for word in positive_words if word in content),
                'negative_count': sum(1 for word in negative_words if word in content),
                'neutral_count': sum(1 for word in neutral_words if word in content),
                'total_words': len(content)
            }
        
        # åˆ†ææ¶ˆæ¯é•¿åº¦åˆ†å¸ƒ
        def analyze_message_length(messages):
            lengths = [len(msg['content']) for msg in messages]
            return {
                'avg_length': sum(lengths) / len(lengths) if lengths else 0,
                'max_length': max(lengths) if lengths else 0,
                'min_length': min(lengths) if lengths else 0,
                'short_messages_ratio': sum(1 for l in lengths if l < 5) / len(lengths) if lengths else 0,
                'long_messages_ratio': sum(1 for l in lengths if l > 20) / len(lengths) if lengths else 0
            }
        
        # åˆ†æå›å¤æ—¶é—´æ¨¡å¼
        def analyze_response_pattern(messages):
            if len(messages) < 2:
                return {'avg_response_time': 0, 'response_consistency': 0}
            
            response_times = []
            for i in range(1, len(messages)):
                time_diff = (messages[i]['timestamp'] - messages[i-1]['timestamp']).total_seconds()
                response_times.append(time_diff)
            
            avg_time = sum(response_times) / len(response_times) if response_times else 0
            std_dev = (sum((t - avg_time) ** 2 for t in response_times) / len(response_times)) ** 0.5 if response_times else 0
            
            return {
                'avg_response_time': avg_time,
                'response_consistency': 1 - (std_dev / avg_time) if avg_time > 0 else 0,
                'quick_responses_ratio': sum(1 for t in response_times if t < 300) / len(response_times) if response_times else 0,
                'slow_responses_ratio': sum(1 for t in response_times if t > 3600) / len(response_times) if response_times else 0
            }
        
        # åˆ†æè¯é¢˜å¤šæ ·æ€§
        def analyze_topic_diversity(messages):
            topics = set()
            for msg in messages:
                content = msg['content'].lower()
                if any(keyword in content for keyword in ['ç”µå½±', 'ç”µè§†å‰§', 'ä¹¦', 'æ–°é—»']):
                    topics.add('entertainment')
                if any(keyword in content for keyword in ['å·¥ä½œ', 'å­¦ä¹ ', 'é¡¹ç›®']):
                    topics.add('work')
                if any(keyword in content for keyword in ['åƒ', 'ç©', 'æ—…è¡Œ']):
                    topics.add('life')
                if any(keyword in content for keyword in ['çˆ±', 'å–œæ¬¢', 'æƒ³']):
                    topics.add('relationship')
            return len(topics)
        
        return {
            'user': {
                'message_count': len(user_messages),
                'word_usage': analyze_word_usage(user_messages),
                'message_length': analyze_message_length(user_messages),
                'response_pattern': analyze_response_pattern(user_messages),
                'topic_diversity': analyze_topic_diversity(user_messages),
                'active_hours': self._analyze_active_hours(user_messages),
                'message_style': self._analyze_message_style(user_messages)
            },
            'partner': {
                'message_count': len(partner_messages),
                'word_usage': analyze_word_usage(partner_messages),
                'message_length': analyze_message_length(partner_messages),
                'response_pattern': analyze_response_pattern(partner_messages),
                'topic_diversity': analyze_topic_diversity(partner_messages),
                'active_hours': self._analyze_active_hours(partner_messages),
                'message_style': self._analyze_message_style(partner_messages)
            }
        }

    def _compare_attitudes(self, before: Dict[str, Any], after: Dict[str, Any]) -> Dict[str, Any]:
        """æ¯”è¾ƒå‰åæ€åº¦å˜åŒ–"""
        return {
            'user': {
                'message_count_change': after['user']['message_count'] - before['user']['message_count'],
                'avg_length_change': after['user']['message_length']['avg_length'] - before['user']['message_length']['avg_length'],
                'response_time_change': after['user']['response_pattern']['avg_response_time'] - before['user']['response_pattern']['avg_response_time'],
                'sentiment_change': after['user']['word_usage']['positive_count'] - before['user']['word_usage']['positive_count']
            },
            'partner': {
                'message_count_change': after['partner']['message_count'] - before['partner']['message_count'],
                'avg_length_change': after['partner']['message_length']['avg_length'] - before['partner']['message_length']['avg_length'],
                'response_time_change': after['partner']['response_pattern']['avg_response_time'] - before['partner']['response_pattern']['avg_response_time'],
                'sentiment_change': after['partner']['word_usage']['positive_count'] - before['partner']['word_usage']['positive_count']
            }
        }

    def extract_landmark_topics(self, messages: List[Dict[str, Any]]):
        """æå–æ ‡å¿—æ€§è¯é¢˜"""
        print("\nå¼€å§‹æå–æ ‡å¿—æ€§è¯é¢˜...")
        self.messages = messages  # ä¿å­˜æ¶ˆæ¯åˆ°å®ä¾‹å±æ€§
        terms_of_endearment = ['å®è´', 'å®å®', 'äº²çˆ±çš„', 'è€å…¬', 'è€å©†', 'äº²çˆ±çš„']
        intimate_keywords = ['æ€§', 'çˆ±', 'äº²å¯†', 'èº«ä½“', 'å…³ç³»']
        
        total_messages = len(messages)
        for i, msg in enumerate(messages, 1):
            if i % 100 == 0:
                print(f"\ræ­£åœ¨å¤„ç†ç¬¬ {i}/{total_messages} æ¡æ¶ˆæ¯... ({(i/total_messages*100):.1f}%)", end="", flush=True)
            
            content = msg['content'].lower()
            sender = 'user' if msg['is_user'] else 'partner'
            
            # æ£€æŸ¥äº²æ˜µç§°å‘¼
            for term in terms_of_endearment:
                if term in content:
                    if not any(t['term'] == term for t in self.landmark_topics['terms_of_endearment']):
                        self.landmark_topics['terms_of_endearment'].append({
                            'term': term,
                            'first_occurrence': msg['timestamp'],
                            'sender': sender
                        })
                        print(f"\nå‘ç°æ–°çš„äº²æ˜µç§°å‘¼: {term} (æ—¶é—´: {msg['timestamp']})")
            
            # æ£€æŸ¥äº²å¯†è¯é¢˜
            if any(keyword in content for keyword in intimate_keywords):
                if not any(t['timestamp'] == msg['timestamp'] for t in self.landmark_topics['intimate_topics']):
                    self.landmark_topics['intimate_topics'].append({
                        'timestamp': msg['timestamp'],
                        'content': content,
                        'sender': sender
                    })
                    print(f"\nå‘ç°äº²å¯†è¯é¢˜è®¨è®º (æ—¶é—´: {msg['timestamp']})")
            
            # æ£€æŸ¥æ–‡åŒ–ç›¸å…³è¯é¢˜
            if re.search(r'ã€Š.*ã€‹|ä½œè€…|ä½œå®¶|å¯¼æ¼”|æ¼”å‘˜|ç”µå½±|ç”µè§†å‰§|æ–°é—»|ç¤¾ä¼š|æ”¿æ²»', content):
                category = 'literature' if re.search(r'ã€Š.*ã€‹|ä½œè€…|ä½œå®¶', content) else \
                          'movies' if re.search(r'ç”µå½±|å¯¼æ¼”|æ¼”å‘˜', content) else \
                          'tv_shows' if re.search(r'ç”µè§†å‰§', content) else \
                          'social_topics'
                
                if not any(t['timestamp'] == msg['timestamp'] for t in self.landmark_topics[category]):
                    self.landmark_topics[category].append({
                        'timestamp': msg['timestamp'],
                        'content': content,
                        'sender': sender
                    })
                    print(f"\nå‘ç°{category}ç›¸å…³è®¨è®º (æ—¶é—´: {msg['timestamp']})")
        
        print("\næ ‡å¿—æ€§è¯é¢˜æå–å®Œæˆï¼")

    def _calculate_avg_response_time(self, messages: List[Dict[str, Any]]) -> float:
        """è®¡ç®—å¹³å‡å›å¤æ—¶é—´"""
        if len(messages) < 2:
            return 0
        
        total_time = 0
        for i in range(1, len(messages)):
            prev_time = messages[i-1]['timestamp']  # ç›´æ¥ä½¿ç”¨æ—¶é—´æˆ³
            curr_time = messages[i]['timestamp']    # ç›´æ¥ä½¿ç”¨æ—¶é—´æˆ³
            total_time += (curr_time - prev_time).total_seconds()
        
        return total_time / (len(messages) - 1)

    def _analyze_sentiment(self, messages: List[Dict[str, Any]]) -> float:
        """åˆ†ææ¶ˆæ¯æƒ…æ„Ÿå€¾å‘"""
        positive_keywords = ['å–œæ¬¢', 'çˆ±', 'å¼€å¿ƒ', 'é«˜å…´', 'å¹¸ç¦', 'å¥½']
        negative_keywords = ['è®¨åŒ', 'ç”Ÿæ°”', 'éš¾è¿‡', 'ä¼¤å¿ƒ', 'ä¸å¥½', 'çƒ¦']
        
        sentiment_score = 0
        for msg in messages:
            content = msg['content']
            for keyword in positive_keywords:
                if keyword in content:
                    sentiment_score += 1
            for keyword in negative_keywords:
                if keyword in content:
                    sentiment_score -= 1
        
        return sentiment_score / len(messages) if messages else 0

    def save_results(self, output_dir: str):
        """ä¿å­˜åˆ†æç»“æœ"""
        print("\næ­£åœ¨ä¿å­˜åˆ†æç»“æœ...")
        
        # è‡ªå®šä¹‰ JSON åºåˆ—åŒ–å‡½æ•°
        def datetime_handler(obj):
            if isinstance(obj, datetime):
                return obj.strftime('%Y-%m-%d %H:%M:%S')
            return obj
        
        results = {
            'key_dates': self.key_dates,
            'landmark_topics': self.landmark_topics,
            'attitude_changes': {
                'relationship_start': self.analyze_attitude_changes(
                    self.messages, 
                    self.key_dates['relationship_start']['date']
                ) if self.key_dates['relationship_start'] else None,
                'conflicts': [
                    self.analyze_attitude_changes(
                        self.messages, 
                        conflict['date']
                    ) for conflict in self.key_dates['conflicts']
                ],
                'special_days': {
                    day_type: self.analyze_attitude_changes(
                        self.messages, 
                        day['date']
                    ) for day_type, days in self.key_dates['special_days'].items()
                    for day in (days if isinstance(days, list) else [days] if days else [])
                }
            }
        }
        
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, 'key_moments_analysis.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=datetime_handler)
        
        print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ‰“å°å…³é”®å‘ç°
        print("\nå…³é”®å‘ç°æ€»ç»“:")
        if self.landmark_topics['terms_of_endearment']:
            print("\näº²æ˜µç§°å‘¼:")
            for term in self.landmark_topics['terms_of_endearment']:
                print(f"- é¦–æ¬¡ä½¿ç”¨ '{term['term']}' åœ¨ {term['first_occurrence']} (ç”± {term['sender']} ä½¿ç”¨)")
        
        if self.landmark_topics['intimate_topics']:
            print("\näº²å¯†è¯é¢˜è®¨è®º:")
            for topic in self.landmark_topics['intimate_topics']:
                print(f"- åœ¨ {topic['timestamp']} è®¨è®º (ç”± {topic['sender']} å‘èµ·)")
        
        for category in ['literature', 'movies', 'tv_shows', 'social_topics']:
            if self.landmark_topics[category]:
                print(f"\n{category}ç›¸å…³è®¨è®º:")
                for topic in self.landmark_topics[category]:
                    print(f"- åœ¨ {topic['timestamp']} è®¨è®º (ç”± {topic['sender']} å‘èµ·)")

    def analyze_key_moments(self):
        """æ·±å…¥åˆ†æå…³é”®æ—¶é—´ç‚¹ä¹‹é—´çš„å…³ç³»å’Œå½±å“"""
        print("\nå¼€å§‹æ·±å…¥åˆ†æå…³é”®æ—¶é—´ç‚¹...")
        
        # 1. åˆ†æå…³ç³»å‘å±•è½¨è¿¹
        relationship_trajectory = self._analyze_relationship_trajectory()
        
        # 2. åˆ†æå†²çªæ¨¡å¼
        conflict_patterns = self._analyze_conflict_patterns()
        
        # 3. åˆ†æç‰¹æ®Šæ—¥å­çš„å½±å“
        special_days_impact = self._analyze_special_days_impact()
        
        # 4. åˆ†æè¯é¢˜æ¼”å˜
        topic_evolution = self._analyze_topic_evolution()
        
        # 5. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
        analysis_report = {
            'relationship_trajectory': relationship_trajectory,
            'conflict_patterns': conflict_patterns,
            'special_days_impact': special_days_impact,
            'topic_evolution': topic_evolution
        }
        
        return analysis_report

    def _analyze_relationship_trajectory(self) -> Dict[str, Any]:
        """åˆ†æå…³ç³»å‘å±•è½¨è¿¹"""
        trajectory = {
            'stages': [],
            'milestones': [],
            'turning_points': []
        }
        
        # åˆ†æå…³ç³»å‘å±•é˜¶æ®µ
        if self.key_dates['relationship_start']:
            start_date = self.key_dates['relationship_start']['date']
            trajectory['stages'].append({
                'stage': 'initial',
                'start_date': start_date,
                'duration': (datetime.now() - start_date).days,
                'characteristics': self._analyze_stage_characteristics(start_date, start_date + timedelta(days=30))
            })
        
        # åˆ†æé‡è¦é‡Œç¨‹ç¢‘
        for conflict in self.key_dates['conflicts']:
            trajectory['milestones'].append({
                'type': 'conflict',
                'date': conflict['date'],
                'impact': self._analyze_conflict_impact(conflict['date'])
            })
        
        # åˆ†æè½¬æŠ˜ç‚¹
        for day_type, days in self.key_dates['special_days'].items():
            for day in (days if isinstance(days, list) else [days] if days else []):
                trajectory['turning_points'].append({
                    'type': day_type,
                    'date': day['date'],
                    'impact': self._analyze_special_day_impact(day['date'])
                })
        
        return trajectory

    def _analyze_conflict_patterns(self) -> Dict[str, Any]:
        """åˆ†æå†²çªæ¨¡å¼"""
        patterns = {
            'frequency': len(self.key_dates['conflicts']),
            'interval_analysis': [],
            'resolution_patterns': [],
            'impact_analysis': []
        }
        
        # åˆ†æå†²çªé—´éš”
        conflicts = sorted(self.key_dates['conflicts'], key=lambda x: x['date'])
        for i in range(1, len(conflicts)):
            interval = (conflicts[i]['date'] - conflicts[i-1]['date']).days
            patterns['interval_analysis'].append({
                'interval': interval,
                'start_date': conflicts[i-1]['date'],
                'end_date': conflicts[i]['date']
            })
        
        # åˆ†æå†²çªè§£å†³æ¨¡å¼
        for conflict in conflicts:
            patterns['resolution_patterns'].append({
                'date': conflict['date'],
                'resolution_time': self._analyze_conflict_resolution(conflict['date']),
                'recovery_pattern': self._analyze_recovery_pattern(conflict['date'])
            })
        
        return patterns

    def _analyze_special_days_impact(self) -> Dict[str, Any]:
        """åˆ†æç‰¹æ®Šæ—¥å­çš„å½±å“"""
        impact = {
            'anniversary': {},
            'valentine': [],
            'qixi': []
        }
        
        # åˆ†æçºªå¿µæ—¥å½±å“
        if self.key_dates['special_days']['anniversary']:
            anniversary = self.key_dates['special_days']['anniversary']
            impact['anniversary'] = {
                'date': anniversary['date'],
                'preparation_pattern': self._analyze_preparation_pattern(anniversary['date']),
                'celebration_pattern': self._analyze_celebration_pattern(anniversary['date']),
                'aftermath_impact': self._analyze_aftermath_impact(anniversary['date'])
            }
        
        # åˆ†ææƒ…äººèŠ‚å½±å“
        for valentine in self.key_dates['special_days']['valentine']:
            impact['valentine'].append({
                'date': valentine['date'],
                'celebration_pattern': self._analyze_celebration_pattern(valentine['date']),
                'gift_pattern': self._analyze_gift_pattern(valentine['date'])
            })
        
        # åˆ†æä¸ƒå¤•èŠ‚å½±å“
        for qixi in self.key_dates['special_days']['qixi']:
            impact['qixi'].append({
                'date': qixi['date'],
                'celebration_pattern': self._analyze_celebration_pattern(qixi['date']),
                'cultural_significance': self._analyze_cultural_significance(qixi['date'])
            })
        
        return impact

    def _analyze_topic_evolution(self) -> Dict[str, Any]:
        """åˆ†æè¯é¢˜æ¼”å˜"""
        evolution = {
            'terms_of_endearment': self._analyze_terms_evolution(),
            'intimate_topics': self._analyze_intimate_topics_evolution(),
            'shared_interests': self._analyze_shared_interests_evolution()
        }
        
        return evolution

    def _analyze_stage_characteristics(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """åˆ†æç‰¹å®šé˜¶æ®µçš„ç‰¹å¾"""
        stage_messages = [msg for msg in self.messages if start_date <= msg['timestamp'] <= end_date]
        return {
            'message_frequency': len(stage_messages) / (end_date - start_date).days if (end_date - start_date).days > 0 else 0,
            'topic_diversity': self._calculate_topic_diversity(stage_messages),
            'emotional_intensity': self._calculate_emotional_intensity(stage_messages),
            'interaction_pattern': self._analyze_interaction_pattern(stage_messages)
        }

    def _analyze_conflict_impact(self, conflict_date: datetime) -> Dict[str, Any]:
        """åˆ†æå†²çªçš„å½±å“"""
        before = self.analyze_attitude_changes(self.messages, conflict_date, days_before=7, days_after=0)
        after = self.analyze_attitude_changes(self.messages, conflict_date, days_before=0, days_after=7)
        
        return {
            'immediate_impact': self._compare_attitudes(before['before'], after['after']),
            'recovery_time': self._calculate_recovery_time(conflict_date),
            'long_term_impact': self._analyze_long_term_impact(conflict_date)
        }

    def _analyze_special_day_impact(self, special_date: datetime) -> Dict[str, Any]:
        """åˆ†æç‰¹æ®Šæ—¥å­çš„å½±å“"""
        return {
            'preparation_behavior': self._analyze_preparation_behavior(special_date),
            'celebration_quality': self._analyze_celebration_quality(special_date),
            'aftermath_effect': self._analyze_aftermath_effect(special_date)
        }

    def _analyze_terms_evolution(self) -> List[Dict[str, Any]]:
        """åˆ†æäº²æ˜µç§°å‘¼çš„æ¼”å˜"""
        evolution = []
        terms = sorted(self.landmark_topics['terms_of_endearment'], key=lambda x: x['first_occurrence'])
        
        for i in range(len(terms)):
            evolution.append({
                'term': terms[i]['term'],
                'first_use': terms[i]['first_occurrence'],
                'usage_frequency': self._calculate_term_frequency(terms[i]['term']),
                'adoption_pattern': self._analyze_term_adoption(terms[i])
            })
        
        return evolution

    def _analyze_intimate_topics_evolution(self) -> List[Dict[str, Any]]:
        """åˆ†æäº²å¯†è¯é¢˜çš„æ¼”å˜"""
        evolution = []
        topics = sorted(self.landmark_topics['intimate_topics'], key=lambda x: x['timestamp'])
        
        for i in range(len(topics)):
            evolution.append({
                'timestamp': topics[i]['timestamp'],
                'topic_depth': self._analyze_topic_depth(topics[i]),
                'discussion_pattern': self._analyze_discussion_pattern(topics[i]),
                'follow_up_impact': self._analyze_follow_up_impact(topics[i])
            })
        
        return evolution

    def _analyze_shared_interests_evolution(self) -> Dict[str, Any]:
        """åˆ†æå…±åŒå…´è¶£çš„æ¼”å˜"""
        evolution = {
            'literature': self._analyze_category_evolution('literature'),
            'movies': self._analyze_category_evolution('movies'),
            'tv_shows': self._analyze_category_evolution('tv_shows'),
            'social_topics': self._analyze_category_evolution('social_topics')
        }
        
        return evolution

    def _analyze_category_evolution(self, category: str) -> List[Dict[str, Any]]:
        """
        åˆ†æç‰¹å®šç±»åˆ«è¯é¢˜çš„æ¼”å˜è¿‡ç¨‹
        
        Args:
            category: è¦åˆ†æçš„è¯é¢˜ç±»åˆ«
            
        Returns:
            åŒ…å«è¯é¢˜æ¼”å˜ä¿¡æ¯çš„åˆ—è¡¨
        """
        evolution = []
        for date in self.key_dates:
            period_messages = self._get_messages_in_period(date)
            category_messages = [msg for msg in period_messages if category in msg.get('category', '')]
            
            if category_messages:
                evolution.append({
                    'date': date,
                    'message_count': len(category_messages),
                    'sentiment': self._calculate_sentiment(category_messages),
                    'key_messages': self._extract_key_messages(category_messages)
                })
        
        return evolution

    def _calculate_topic_diversity(self, messages: List[Dict[str, Any]]) -> float:
        """è®¡ç®—è¯é¢˜å¤šæ ·æ€§"""
        topics = set()
        for msg in messages:
            content = msg['content'].lower()
            if any(keyword in content for keyword in ['ç”µå½±', 'ç”µè§†å‰§', 'ä¹¦', 'æ–°é—»']):
                topics.add('entertainment')
            if any(keyword in content for keyword in ['å·¥ä½œ', 'å­¦ä¹ ', 'é¡¹ç›®']):
                topics.add('work')
            if any(keyword in content for keyword in ['åƒ', 'ç©', 'æ—…è¡Œ']):
                topics.add('life')
        return len(topics) / len(messages) if messages else 0

    def _calculate_emotional_intensity(self, messages: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æƒ…æ„Ÿå¼ºåº¦"""
        intensity = 0
        for msg in messages:
            content = msg['content']
            # è®¡ç®—æƒ…æ„Ÿè¯å¯†åº¦
            emotional_words = sum(1 for word in ['å–œæ¬¢', 'çˆ±', 'å¼€å¿ƒ', 'éš¾è¿‡', 'ç”Ÿæ°”'] if word in content)
            intensity += emotional_words / len(content) if content else 0
        return intensity / len(messages) if messages else 0

    def _analyze_interaction_pattern(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æäº’åŠ¨æ¨¡å¼"""
        user_messages = [msg for msg in messages if msg['is_user']]
        partner_messages = [msg for msg in messages if not msg['is_user']]
        
        return {
            'message_ratio': len(user_messages) / len(partner_messages) if partner_messages else 0,
            'response_time': self._calculate_avg_response_time(messages),
            'conversation_depth': self._calculate_conversation_depth(messages)
        }

    def _calculate_conversation_depth(self, messages: List[Dict[str, Any]]) -> float:
        """è®¡ç®—å¯¹è¯æ·±åº¦"""
        depth = 0
        current_depth = 0
        for msg in messages:
            if msg['is_user']:
                current_depth += 1
                depth = max(depth, current_depth)
            else:
                current_depth = max(0, current_depth - 1)
        return depth

    def _calculate_term_frequency(self, term: str) -> float:
        """è®¡ç®—ç§°å‘¼ä½¿ç”¨é¢‘ç‡"""
        total_messages = len(self.messages)
        term_count = sum(1 for msg in self.messages if term in msg['content'])
        return term_count / total_messages if total_messages > 0 else 0

    def _analyze_term_adoption(self, term: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç§°å‘¼é‡‡ç”¨æ¨¡å¼"""
        return {
            'initial_usage': self._analyze_initial_usage(term),
            'adoption_speed': self._analyze_adoption_speed(term),
            'usage_consistency': self._analyze_usage_consistency(term)
        }

    def _analyze_topic_depth(self, topic: Dict[str, Any]) -> int:
        """åˆ†æè¯é¢˜æ·±åº¦"""
        related_messages = [msg for msg in self.messages 
                          if abs((msg['timestamp'] - topic['timestamp']).total_seconds()) < 3600]
        return len(related_messages)

    def _analyze_discussion_pattern(self, topic: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè®¨è®ºæ¨¡å¼"""
        return {
            'initiation': self._analyze_topic_initiation(topic),
            'development': self._analyze_topic_development(topic),
            'conclusion': self._analyze_topic_conclusion(topic)
        }

    def _analyze_follow_up_impact(self, topic: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æåç»­å½±å“"""
        return {
            'immediate_impact': self._analyze_immediate_impact(topic),
            'long_term_impact': self._analyze_long_term_impact(topic['timestamp'])
        }

    def _analyze_initial_usage(self, term: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç§°å‘¼åˆå§‹ä½¿ç”¨"""
        return {
            'first_use': term['first_occurrence'],
            'usage_frequency': self._calculate_term_frequency(term['term'])
        }

    def _analyze_adoption_speed(self, term: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç§°å‘¼é‡‡ç”¨é€Ÿåº¦"""
        return {
            'adoption_speed': self._calculate_term_frequency(term['term'])
        }

    def _analyze_usage_consistency(self, term: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç§°å‘¼ä½¿ç”¨ä¸€è‡´æ€§"""
        return {
            'usage_consistency': self._calculate_term_frequency(term['term'])
        }

    def _analyze_topic_initiation(self, topic: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¯é¢˜å‘èµ·"""
        return {
            'initiation_pattern': self._analyze_discussion_pattern(topic)
        }

    def _analyze_topic_development(self, topic: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¯é¢˜å‘å±•"""
        return {
            'topic_development': self._calculate_topic_diversity(
                [msg for msg in self.messages if abs((msg['timestamp'] - topic['timestamp']).total_seconds()) < 3600]
            )
        }

    def _analyze_topic_conclusion(self, topic: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¯é¢˜ç»“è®º"""
        return {
            'conclusion_pattern': self._calculate_emotional_intensity(
                [msg for msg in self.messages if abs((msg['timestamp'] - topic['timestamp']).total_seconds()) < 3600]
            )
        }

    def _analyze_immediate_impact(self, topic: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç›´æ¥å½±å“"""
        return {
            'immediate_impact': self._calculate_emotional_intensity(
                [msg for msg in self.messages if abs((msg['timestamp'] - topic['timestamp']).total_seconds()) < 3600]
            )
        }

    def _analyze_long_term_impact(self, date: datetime) -> Dict[str, Any]:
        """åˆ†æé•¿æœŸå½±å“"""
        return {
            'long_term_impact': self._calculate_emotional_intensity(
                [msg for msg in self.messages if msg['timestamp'] <= date]
            )
        }

    def _analyze_preparation_pattern(self, date: datetime) -> Dict[str, Any]:
        """åˆ†æå‡†å¤‡æ¨¡å¼"""
        return {
            'preparation_pattern': self._calculate_emotional_intensity(
                [msg for msg in self.messages if msg['timestamp'] <= date]
            )
        }

    def _analyze_celebration_pattern(self, date: datetime) -> Dict[str, Any]:
        """åˆ†æåº†ç¥æ¨¡å¼"""
        return {
            'celebration_pattern': self._calculate_emotional_intensity(
                [msg for msg in self.messages if msg['timestamp'] <= date]
            )
        }

    def _analyze_aftermath_impact(self, date: datetime) -> Dict[str, Any]:
        """åˆ†æåç»­å½±å“"""
        return {
            'aftermath_impact': self._calculate_emotional_intensity(
                [msg for msg in self.messages if msg['timestamp'] > date]
            )
        }

    def _analyze_gift_pattern(self, date: datetime) -> Dict[str, Any]:
        """åˆ†æç¤¼ç‰©æ¨¡å¼"""
        return {
            'gift_pattern': self._calculate_emotional_intensity(
                [msg for msg in self.messages if msg['timestamp'] <= date]
            )
        }

    def _analyze_cultural_significance(self, date: datetime) -> Dict[str, Any]:
        """åˆ†ææ–‡åŒ–æ„ä¹‰"""
        return {
            'cultural_significance': self._calculate_emotional_intensity(
                [msg for msg in self.messages if msg['timestamp'] <= date]
            )
        }

    def _analyze_preparation_behavior(self, date: datetime) -> Dict[str, Any]:
        """åˆ†æå‡†å¤‡è¡Œä¸º"""
        return {
            'preparation_behavior': self._calculate_emotional_intensity(
                [msg for msg in self.messages if msg['timestamp'] <= date]
            )
        }

    def _analyze_celebration_quality(self, date: datetime) -> Dict[str, Any]:
        """åˆ†æåº†ç¥è´¨é‡"""
        return {
            'celebration_quality': self._calculate_emotional_intensity(
                [msg for msg in self.messages if msg['timestamp'] <= date]
            )
        }

    def _analyze_aftermath_effect(self, date: datetime) -> Dict[str, Any]:
        """åˆ†æåç»­æ•ˆæœ"""
        return {
            'aftermath_effect': self._calculate_emotional_intensity(
                [msg for msg in self.messages if msg['timestamp'] > date]
            )
        }

    def _analyze_conflict_resolution(self, date: datetime) -> Dict[str, Any]:
        """åˆ†æå†²çªè§£å†³"""
        return {
            'resolution_time': self._calculate_avg_response_time(
                [msg for msg in self.messages if msg['timestamp'] <= date]
            )
        }

    def _analyze_recovery_pattern(self, date: datetime) -> Dict[str, Any]:
        """åˆ†ææ¢å¤æ¨¡å¼"""
        return {
            'recovery_pattern': self._calculate_emotional_intensity(
                [msg for msg in self.messages if msg['timestamp'] > date]
            )
        }

    def _calculate_recovery_time(self, date: datetime) -> Dict[str, Any]:
        """è®¡ç®—æ¢å¤æ—¶é—´"""
        return {
            'recovery_time': self._calculate_avg_response_time(
                [msg for msg in self.messages if msg['timestamp'] > date]
            )
        }

    def _get_messages_in_period(self, date):
        return [msg for msg in self.messages if date <= msg['timestamp'] <= date + timedelta(days=1)]

    def _calculate_sentiment(self, messages):
        positive_keywords = ['å–œæ¬¢', 'çˆ±', 'å¼€å¿ƒ', 'é«˜å…´', 'å¹¸ç¦', 'å¥½']
        negative_keywords = ['è®¨åŒ', 'ç”Ÿæ°”', 'éš¾è¿‡', 'ä¼¤å¿ƒ', 'ä¸å¥½', 'çƒ¦']
        
        sentiment_score = 0
        for msg in messages:
            content = msg['content']
            for keyword in positive_keywords:
                if keyword in content:
                    sentiment_score += 1
            for keyword in negative_keywords:
                if keyword in content:
                    sentiment_score -= 1
        
        return sentiment_score / len(messages) if messages else 0

    def _extract_key_messages(self, messages):
        return [msg['content'] for msg in messages if msg['is_user']]

    def _analyze_active_hours(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ´»è·ƒæ—¶é—´æ®µ"""
        hour_counts = [0] * 24
        for msg in messages:
            hour = msg['timestamp'].hour
            hour_counts[hour] += 1
        
        # è®¡ç®—æœ€æ´»è·ƒçš„æ—¶æ®µ
        max_hour = hour_counts.index(max(hour_counts))
        active_hours = [i for i, count in enumerate(hour_counts) if count > sum(hour_counts) / 24]
        
        return {
            'most_active_hour': max_hour,
            'active_hours': active_hours,
            'morning_activity': sum(hour_counts[6:12]) / sum(hour_counts) if sum(hour_counts) > 0 else 0,
            'afternoon_activity': sum(hour_counts[12:18]) / sum(hour_counts) if sum(hour_counts) > 0 else 0,
            'evening_activity': sum(hour_counts[18:24]) / sum(hour_counts) if sum(hour_counts) > 0 else 0,
            'night_activity': sum(hour_counts[0:6]) / sum(hour_counts) if sum(hour_counts) > 0 else 0
        }

    def _analyze_message_style(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ¶ˆæ¯é£æ ¼"""
        # è¡¨æƒ…ç¬¦å·åˆ†æ
        emoji_patterns = {
            'happy': ['ğŸ˜Š', 'ğŸ˜„', 'ğŸ˜‚', 'ğŸ˜', 'ğŸ˜˜'],
            'sad': ['ğŸ˜¢', 'ğŸ˜­', 'ğŸ˜”', 'ğŸ˜'],
            'angry': ['ğŸ˜ ', 'ğŸ˜¡', 'ğŸ˜¤'],
            'neutral': ['ğŸ˜', 'ğŸ™‚', 'ğŸ˜¶']
        }
        
        # æ ‡ç‚¹ç¬¦å·åˆ†æ
        punctuation_patterns = {
            'exclamation': ['!', 'ï¼'],
            'question': ['?', 'ï¼Ÿ'],
            'ellipsis': ['...', 'â€¦'],
            'period': ['.', 'ã€‚']
        }
        
        def count_patterns(text, patterns):
            return {key: sum(1 for p in patterns[key] if p in text) for key in patterns}
        
        # æ¶ˆæ¯ç±»å‹åˆ†æ
        def analyze_message_types(messages):
            types = {
                'text_only': 0,
                'with_emoji': 0,
                'with_image': 0,
                'with_link': 0,
                'with_voice': 0
            }
            
            for msg in messages:
                content = msg['content']
                if 'http' in content:
                    types['with_link'] += 1
                elif '[å›¾ç‰‡]' in content:
                    types['with_image'] += 1
                elif '[è¯­éŸ³]' in content:
                    types['with_voice'] += 1
                elif any(emoji in content for emoji_list in emoji_patterns.values() for emoji in emoji_list):
                    types['with_emoji'] += 1
                else:
                    types['text_only'] += 1
            
            total = sum(types.values())
            return {k: v/total if total > 0 else 0 for k, v in types.items()}
        
        # åˆ†ææ‰€æœ‰æ¶ˆæ¯
        all_content = ' '.join(msg['content'] for msg in messages)
        
        return {
            'emoji_usage': count_patterns(all_content, emoji_patterns),
            'punctuation_usage': count_patterns(all_content, punctuation_patterns),
            'message_types': analyze_message_types(messages),
            'avg_words_per_message': len(all_content.split()) / len(messages) if messages else 0,
            'unique_words_ratio': len(set(all_content.split())) / len(all_content.split()) if all_content else 0
        }